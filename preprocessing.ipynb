{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/huggingface/transformers/blob/master/examples/contrib/mm-imdb/utils_mmimdb.py\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "POOLING_BREAKDOWN = {1: (1, 1), 2: (2, 1), 3: (3, 1), 4: (2, 2), 5: (5, 1), 6: (3, 2), 7: (7, 1), 8: (4, 2), 9: (3, 3)}\n",
    "class ImageEncoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        model = torchvision.models.resnet152(pretrained=True)\n",
    "        modules = list(model.children())[:-2] # remove last two layers\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.pool = nn.AdaptiveAvgPool2d(POOLING_BREAKDOWN[args.num_image_embeds])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Bx3x224x224 -> Bx2048x7x7 -> Bx2048xN -> BxNx2048\n",
    "        out = self.pool(self.model(x))\n",
    "        out = torch.flatten(out, start_dim=2)\n",
    "        out = out.transpose(1, 2).contiguous()\n",
    "        return out  # BxNx2048\n",
    "    \n",
    "class JsonlDataset(Dataset):\n",
    "    def __init__(self, data_path, tokenizer, transforms, labels, max_seq_length, \n",
    "                 image_only=False, text_only=False, use_transformed_tensors=False):\n",
    "        self.data = [json.loads(l) for l in open(data_path)]\n",
    "        self.data_dir = os.path.dirname(data_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.labels = labels\n",
    "        self.n_classes = len(labels)\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_only=image_only\n",
    "        self.text_only=text_only\n",
    "        self.use_transformed_tensors=use_transformed_tensors\n",
    "\n",
    "        self.classes=[\n",
    "            \"Crime\",\n",
    "            \"Drama\",\n",
    "            \"Thriller\",\n",
    "            \"Action\",\n",
    "            \"Comedy\",\n",
    "            \"Romance\",\n",
    "            \"Documentary\",\n",
    "            \"Short\",\n",
    "            \"Mystery\",\n",
    "            \"History\",\n",
    "            \"Family\",\n",
    "            \"Adventure\",\n",
    "            \"Fantasy\",\n",
    "            \"Sci-Fi\",\n",
    "            \"Western\",\n",
    "            \"Horror\",\n",
    "            \"Sport\",\n",
    "            \"War\",\n",
    "            \"Music\",\n",
    "            \"Musical\",\n",
    "            \"Animation\",\n",
    "            \"Biography\",\n",
    "            \"Film-Noir\",\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "  \n",
    "    def get_sentence(self, index):\n",
    "        sentence = torch.LongTensor(self.tokenizer.encode(self.data[index][\"text\"],\n",
    "                                                          add_special_tokens=True,\n",
    "                                                          max_length=args.max_seq_length))\n",
    "        #start_token, sentence, end_token = sentence[0], sentence[1:-1], sentence[-1]\n",
    "        #sentence = sentence[: self.max_seq_length]\n",
    "        return sentence\n",
    "  \n",
    "    def get_image(self, index):\n",
    "        if self.use_transformed_tensors:\n",
    "            id_ = self.data[index][\"img\"].split('.')[0]\n",
    "            return torch.load(os.path.join(self.data_dir, '{}.pt'.format(id_)))\n",
    "        \n",
    "        image = Image.open(os.path.join(self.data_dir, self.data[index][\"img\"])).convert(\"RGB\")\n",
    "        image = self.transforms(image)\n",
    "        return image\n",
    "      \n",
    "    def __getitem__(self, index):\n",
    "        label = torch.zeros(self.n_classes)\n",
    "        label[[self.labels.index(tgt) for tgt in self.data[index][\"label\"]]] = 1\n",
    "\n",
    "        if self.image_only:  \n",
    "            image = self.get_image(index)\n",
    "            id_ = self.data[index][\"img\"].split('.')[0]\n",
    "            return {\"image\": image, \"id\": id_,\"label\": label}\n",
    "        elif self.text_only:\n",
    "            sentence = self.get_sentence(index)\n",
    "            return {\"sentence\": sentence, \"label\": label}\n",
    "\n",
    "        sentence = self.get_sentence(index)\n",
    "        image = self.get_image(index)\n",
    "\n",
    "        return {\n",
    "            #\"image_start_token\": start_token,\n",
    "            #\"image_end_token\": end_token,\n",
    "            \"sentence\": sentence,\n",
    "            \"image\": image,\n",
    "            \"label\": label,\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_label_frequencies(self):\n",
    "        label_freqs = Counter()\n",
    "        for row in self.data:\n",
    "            label_freqs.update(row[\"label\"])\n",
    "        return label_freqs\n",
    "\n",
    "def get_mmimdb_labels():\n",
    "      return [\n",
    "        \"Crime\",\n",
    "        \"Drama\",\n",
    "        \"Thriller\",\n",
    "        \"Action\",\n",
    "        \"Comedy\",\n",
    "        \"Romance\",\n",
    "        \"Documentary\",\n",
    "        \"Short\",\n",
    "        \"Mystery\",\n",
    "        \"History\",\n",
    "        \"Family\",\n",
    "        \"Adventure\",\n",
    "        \"Fantasy\",\n",
    "        \"Sci-Fi\",\n",
    "        \"Western\",\n",
    "        \"Horror\",\n",
    "        \"Sport\",\n",
    "        \"War\",\n",
    "        \"Music\",\n",
    "        \"Musical\",\n",
    "        \"Animation\",\n",
    "        \"Biography\",\n",
    "        \"Film-Noir\",\n",
    "      ]\n",
    "\n",
    "\n",
    "def get_image_transforms():\n",
    "  return transforms.Compose(\n",
    "    [\n",
    "      transforms.Resize(256),\n",
    "      transforms.CenterCrop(224),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.46777044, 0.44531429, 0.40661017], std=[0.12221994, 0.12145835, 0.14380469],),\n",
    "    ]\n",
    "  )\n",
    "    \n",
    "def load_examples(args, tokenizer, split='train'):\n",
    "    path = os.path.join(args.data_dir, '{}.jsonl'.format(split))\n",
    "    transforms = get_image_transforms()\n",
    "    labels = get_mmimdb_labels()\n",
    "    dataset = JsonlDataset(path, tokenizer,\n",
    "                           transforms, labels, \n",
    "                           args.max_seq_length - 2,\n",
    "                           image_only=args.image_only, \n",
    "                           text_only=args.text_only,\n",
    "                           use_transformed_tensors=args.use_transformed_tensors\n",
    "                          )\n",
    "    return dataset\n",
    "\n",
    "class Args:\n",
    "    def __init__(self,\n",
    "                 data_dir='/home/miaortizma/work/datanfs/mmimdb/dataset',\n",
    "                 output_dir='/homee/miaortizma/work/datanfs/home/miaortizma/mm_output',\n",
    "                 model_name_or_path='my_model',\n",
    "                 tokenizer_name='bert-base-uncased',\n",
    "                 num_image_embeds=1,\n",
    "                 max_seq_length=128,\n",
    "                 image_only=False,\n",
    "                 text_only=False,\n",
    "                 per_gpu_train_batch_size=8,\n",
    "                 num_train_epochs=100,\n",
    "                 patience=5,\n",
    "                 gradient_accumulation_steps=20,\n",
    "                 use_transformed_tensors=False,\n",
    "                ):\n",
    "        \n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "\n",
    "        self.model_name_or_path=model_name_or_path\n",
    "        self.tokenizer_name=tokenizer_name\n",
    "\n",
    "        self.num_image_embeds=num_image_embeds\n",
    "        self.max_seq_length=max_seq_length\n",
    "\n",
    "        self.num_train_epochs=num_train_epochs\n",
    "        self.gradient_accumulation_steps=gradient_accumulation_steps\n",
    "        self.patience=patience\n",
    "\n",
    "        self.image_only = image_only\n",
    "        self.text_only = text_only\n",
    "        self.use_transformed_tensors=use_transformed_tensors\n",
    "\n",
    "        self.seed=0\n",
    "        self.n_gpu=1\n",
    "        self.do_lower_case=True\n",
    "        #self.cache_dir=None\n",
    "        #self.max_steps=-1\n",
    "        self.dropout_prob=0.5\n",
    "        #self.weight_decay=0.0\n",
    "        #self.learning_rate=5e-5\n",
    "        #self.adam_epsilon=1e-8\n",
    "        #self.max_grad_norm=1.0\n",
    "        #self.warmup_steps=0\n",
    "        #self.num_workers=0\n",
    "        labels = get_mmimdb_labels()\n",
    "        num_labels = len(labels)\n",
    "        self.num_labels = num_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn_image_only(batch):\n",
    "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
    "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
    "    return img_tensor, tgt_tensor\n",
    "\n",
    "def collate_fn_text_only(batch):\n",
    "    lens = [len(row[\"sentence\"]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "\n",
    "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
    "\n",
    "    return (text_tensor, mask_tensor), tgt_tensor\n",
    "\n",
    "def collate_fn_modal(batch):\n",
    "    \n",
    "    lens = [len(row[\"sentence\"]) for row in batch]\n",
    "    bsz, max_seq_len = len(batch), max(lens)\n",
    "\n",
    "    mask_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "    text_tensor = torch.zeros(bsz, max_seq_len, dtype=torch.long)\n",
    "\n",
    "    for i_batch, (input_row, length) in enumerate(zip(batch, lens)):\n",
    "        text_tensor[i_batch, :length] = input_row[\"sentence\"]\n",
    "        mask_tensor[i_batch, :length] = 1\n",
    "        \n",
    "    img_tensor = torch.stack([row[\"image\"] for row in batch])\n",
    "    tgt_tensor = torch.stack([row[\"label\"] for row in batch])\n",
    "    \n",
    "    return (text_tensor, mask_tensor, img_tensor), tgt_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aafdf1085de4c69b782da3434c24b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=15552.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "def transform_tensors(split):\n",
    "    args = Args(image_only=True)\n",
    "    dataset = load_examples(args, None, split=split)\n",
    "    for data in tqdm(dataset):    \n",
    "        tensor_path = args.data_dir + '/{}.pt'.format(data['id'])\n",
    "        torch.save(data['image'], tensor_path)\n",
    "        \n",
    "#transform_tensors('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca4cd0d489804c7c9665dfa205fe06ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2608.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#transform_tensors('val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171edf7047a647e580d3b241642550f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7799.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (92984898 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/TiffImagePlugin.py:788: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (100419400 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n",
      "/opt/conda/lib/python3.7/site-packages/PIL/Image.py:2766: DecompressionBombWarning: Image size (101769871 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  DecompressionBombWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#transform_tensors('test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
